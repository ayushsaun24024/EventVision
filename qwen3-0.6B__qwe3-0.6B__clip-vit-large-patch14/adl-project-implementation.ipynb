{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INSTALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:55:57.193546Z",
     "iopub.status.busy": "2025-11-13T07:55:57.192730Z",
     "iopub.status.idle": "2025-11-13T07:56:00.767351Z",
     "shell.execute_reply": "2025-11-13T07:56:00.766313Z",
     "shell.execute_reply.started": "2025-11-13T07:55:57.193517Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install -q faiss-cpu sentence-transformers transformers torch torchvision accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:56:00.769528Z",
     "iopub.status.busy": "2025-11-13T07:56:00.769271Z",
     "iopub.status.idle": "2025-11-13T07:56:00.775888Z",
     "shell.execute_reply": "2025-11-13T07:56:00.775079Z",
     "shell.execute_reply.started": "2025-11-13T07:56:00.769503Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import torch\n",
    "import faiss\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "from transformers import (\n",
    "    CLIPProcessor, CLIPModel,\n",
    "    AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONSTANTS & CONFIG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:56:00.776898Z",
     "iopub.status.busy": "2025-11-13T07:56:00.776701Z",
     "iopub.status.idle": "2025-11-13T07:56:00.798400Z",
     "shell.execute_reply": "2025-11-13T07:56:00.797645Z",
     "shell.execute_reply.started": "2025-11-13T07:56:00.776883Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    DATASET_PATH: str = './Dataset'\n",
    "    DATABASE_JSON: str = f'{DATASET_PATH}/database.json'\n",
    "    TRAIN_CSV: str = f'{DATASET_PATH}/train_set.csv'\n",
    "    TEST_CSV: str = f'{DATASET_PATH}/test_public.csv'\n",
    "    IMAGE_DIR: str = f'{DATASET_PATH}/database_images_compressed90'\n",
    "\n",
    "    PRECOMPUTED_PATH: str = './eventa_embeddings_Qwen3'\n",
    "    EMBEDDINGS_FILE: str = f'{PRECOMPUTED_PATH}/database_embeddings_Qwen3.npy'\n",
    "    ARTICLE_IDS_FILE: str = f'{PRECOMPUTED_PATH}/database_article_ids_Qwen3.npy'\n",
    "    FAISS_INDEX_FILE: str = f'{PRECOMPUTED_PATH}/database_faiss_index_Qwen3.bin'\n",
    "\n",
    "    EMBEDDING_MODEL: str = 'Qwen/Qwen3-Embedding-0.6B'\n",
    "    ARTICLE_RERANKER_MODEL: str = 'Qwen/Qwen3-Reranker-0.6B'\n",
    "    CLIP_MODEL: str = \"openai/clip-vit-large-patch14\"\n",
    "\n",
    "    DEVICE: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    EMBEDDING_BATCH_SIZE: int = 64\n",
    "    RERANK_BATCH_SIZE: int = 64\n",
    "    IMG_BATCH: int = 32\n",
    "\n",
    "    TOP_K_ARTICLES: int = 100\n",
    "    TOP_K_ARTICLES_RERANK: int = 20\n",
    "    TOP_K_IMAGES: int = 10\n",
    "\n",
    "    MAX_RERANKER_LENGTH: int = 4096\n",
    "    MAX_DOC_CHARS: int = 2000\n",
    "    MAX_CLIP_TOKENS: int = 75\n",
    "\n",
    "    TRAIN_VAL_SPLIT: float = 0.999\n",
    "    RANDOM_SEED: int = 42\n",
    "\n",
    "config = Config()\n",
    "print(\"Device:\", config.DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTILITY FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:56:00.800285Z",
     "iopub.status.busy": "2025-11-13T07:56:00.799977Z",
     "iopub.status.idle": "2025-11-13T07:56:00.815857Z",
     "shell.execute_reply": "2025-11-13T07:56:00.815009Z",
     "shell.execute_reply.started": "2025-11-13T07:56:00.800266Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_json(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def train_val_split(df, split_ratio=0.9, seed=42):\n",
    "    train = df.sample(frac=split_ratio, random_state=seed)\n",
    "    val = df.drop(train.index)\n",
    "    return train.reset_index(drop=True), val.reset_index(drop=True)\n",
    "\n",
    "def normalize_embeddings(E):\n",
    "    E = E.astype(np.float32)\n",
    "    faiss.normalize_L2(E)\n",
    "    return E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METRIC FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:56:00.817157Z",
     "iopub.status.busy": "2025-11-13T07:56:00.816789Z",
     "iopub.status.idle": "2025-11-13T07:56:00.841732Z",
     "shell.execute_reply": "2025-11-13T07:56:00.840871Z",
     "shell.execute_reply.started": "2025-11-13T07:56:00.817134Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_recall_at_k(pred, gt, k):\n",
    "    hit = sum(1 for p, g in zip(pred, gt) if g in p[:k])\n",
    "    return hit / len(gt)\n",
    "\n",
    "def compute_mrr(pred, gt):\n",
    "    s = []\n",
    "    for p, g in zip(pred, gt):\n",
    "        s.append(1/(p.index(g)+1) if g in p else 0)\n",
    "    return sum(s)/len(gt)\n",
    "\n",
    "def compute_map(pred, gt):\n",
    "    s = []\n",
    "    for p, g in zip(pred, gt):\n",
    "        s.append(1/(p.index(g)+1) if g in p else 0)\n",
    "    return sum(s)/len(gt)\n",
    "\n",
    "def evaluate_retrieval(pred, gt, name):\n",
    "    print(f\"\\n{name} Metrics:\")\n",
    "    metrics = {\n",
    "        \"mAP\": compute_map(pred, gt),\n",
    "        \"MRR\": compute_mrr(pred, gt),\n",
    "        \"Recall@1\": compute_recall_at_k(pred, gt, 1),\n",
    "        \"Recall@5\": compute_recall_at_k(pred, gt, 5),\n",
    "        \"Recall@10\": compute_recall_at_k(pred, gt, 10),\n",
    "        \"Recall@20\": compute_recall_at_k(pred, gt, 20),\n",
    "        \"Recall@50\": compute_recall_at_k(pred, gt, 50),\n",
    "    }\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATALOADER/DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:56:00.842979Z",
     "iopub.status.busy": "2025-11-13T07:56:00.842704Z",
     "iopub.status.idle": "2025-11-13T07:56:30.060839Z",
     "shell.execute_reply": "2025-11-13T07:56:30.059890Z",
     "shell.execute_reply.started": "2025-11-13T07:56:00.842940Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "database = load_json(config.DATABASE_JSON)\n",
    "train_df = pd.read_csv(config.TRAIN_CSV)\n",
    "train_df, val_df = train_val_split(train_df, config.TRAIN_VAL_SPLIT, config.RANDOM_SEED)\n",
    "\n",
    "article_image_map = defaultdict(list)\n",
    "for aid, data in database.items():\n",
    "    imgs = data.get(\"images\", []) or []\n",
    "    for img in imgs:\n",
    "        if isinstance(img, str):\n",
    "            iid = os.path.splitext(os.path.basename(img))[0]\n",
    "            article_image_map[aid].append(iid)\n",
    "            continue\n",
    "        if isinstance(img, dict):\n",
    "            for key in [\"image_id\", \"id\", \"file_name\", \"filename\", \"path\", \"file\"]:\n",
    "                if key in img:\n",
    "                    iid = os.path.splitext(os.path.basename(img[key]))[0]\n",
    "                    article_image_map[aid].append(iid)\n",
    "                    break\n",
    "\n",
    "print(\"Train:\", len(train_df), \"Val:\", len(val_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARCHITECTURE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMBEDDING & FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:56:30.062082Z",
     "iopub.status.busy": "2025-11-13T07:56:30.061778Z",
     "iopub.status.idle": "2025-11-13T07:56:30.081415Z",
     "shell.execute_reply": "2025-11-13T07:56:30.080598Z",
     "shell.execute_reply.started": "2025-11-13T07:56:30.062056Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_embedding_model():\n",
    "    m = SentenceTransformer(config.EMBEDDING_MODEL, device=config.DEVICE, trust_remote_code=True)\n",
    "    if config.DEVICE == 'cuda':\n",
    "        m.half()\n",
    "    return m\n",
    "\n",
    "def generate_embeddings(model, texts, batch=64):\n",
    "    out = []\n",
    "    for i in tqdm(range(0, len(texts), batch)):\n",
    "        b = model.encode(texts[i:i+batch], convert_to_numpy=True)\n",
    "        out.append(b)\n",
    "    return np.vstack(out)\n",
    "\n",
    "def search_index(index, Q, k):\n",
    "    Q = normalize_embeddings(Q)\n",
    "    dist, idx = index.search(Q, k)\n",
    "    return dist, idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QWEN3 RERANKER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:56:30.082431Z",
     "iopub.status.busy": "2025-11-13T07:56:30.082178Z",
     "iopub.status.idle": "2025-11-13T07:56:48.559134Z",
     "shell.execute_reply": "2025-11-13T07:56:48.558505Z",
     "shell.execute_reply.started": "2025-11-13T07:56:30.082405Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_qwen3_reranker():\n",
    "    tok = AutoTokenizer.from_pretrained(config.ARTICLE_RERANKER_MODEL, trust_remote_code=True, padding_side='left')\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.ARTICLE_RERANKER_MODEL,\n",
    "        torch_dtype=torch.float16 if config.DEVICE=='cuda' else torch.float32,\n",
    "        trust_remote_code=True\n",
    "    ).to(config.DEVICE)\n",
    "    model.eval()\n",
    "    prefix = \"<|im_start|>system\\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \\\"yes\\\" or \\\"no\\\".\\n<|im_end|>\\n<|im_start|>user\\n\"\n",
    "    suffix = \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\n",
    "    pre = tok.encode(prefix, add_special_tokens=False)\n",
    "    suf = tok.encode(suffix, add_special_tokens=False)\n",
    "    yes_id = tok.convert_tokens_to_ids(\"yes\") or tok.convert_tokens_to_ids(\"Yes\")\n",
    "    no_id  = tok.convert_tokens_to_ids(\"no\")  or tok.convert_tokens_to_ids(\"No\")\n",
    "    return tok, model, pre, suf, yes_id, no_id\n",
    "\n",
    "reranker_tokenizer, reranker_model, prefix_tokens, suffix_tokens, yes_id, no_id = load_qwen3_reranker()\n",
    "\n",
    "def format_instruction(query, doc):\n",
    "    return f\"<Instruct>: Determine relevance\\n<Query>: {query}\\n<Document>: {doc}\"\n",
    "\n",
    "def process_inputs(pairs):\n",
    "    toks = reranker_tokenizer(pairs, add_special_tokens=False, padding=False, truncation='longest_first')\n",
    "    for i, ids in enumerate(toks['input_ids']):\n",
    "        toks['input_ids'][i] = prefix_tokens + ids + suffix_tokens\n",
    "    toks = reranker_tokenizer.pad(\n",
    "        {\"input_ids\": toks['input_ids']},\n",
    "        padding=True, return_tensors=\"pt\",\n",
    "        max_length=config.MAX_RERANKER_LENGTH\n",
    "    )\n",
    "    return {k: v.to(config.DEVICE) for k, v in toks.items()}\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_yes_scores(inputs):\n",
    "    logits = reranker_model(**inputs).logits[:, -1, :]\n",
    "    pair = torch.stack([logits[:, no_id], logits[:, yes_id]], dim=1)\n",
    "    probs = torch.nn.functional.log_softmax(pair, dim=1)\n",
    "    return probs[:,1].exp().cpu().tolist()\n",
    "\n",
    "def rerank_articles(query, article_ids, batch_size=4):\n",
    "    pairs = []\n",
    "    for aid in article_ids:\n",
    "        art = database[aid]\n",
    "        title = art.get(\"title\", \"\")[:200]\n",
    "        content = art.get(\"content\", \"\")[:config.MAX_DOC_CHARS]\n",
    "        doc = f\"Title: {title}\\nContent: {content}\"\n",
    "        pairs.append(format_instruction(query, doc))\n",
    "    scores = []\n",
    "    for i in range(0, len(pairs), batch_size):\n",
    "        batch_pairs = pairs[i:i+batch_size]\n",
    "        inp = process_inputs(batch_pairs)\n",
    "        s = compute_yes_scores(inp)\n",
    "        scores.extend(s)\n",
    "        torch.cuda.empty_cache()\n",
    "    ranked = sorted(zip(article_ids, scores), key=lambda x: x[1], reverse=True)\n",
    "    return [a for a,_ in ranked[:config.TOP_K_ARTICLES_RERANK]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMAGE RERANKER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:56:48.560539Z",
     "iopub.status.busy": "2025-11-13T07:56:48.560287Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "clip_model = CLIPModel.from_pretrained(config.CLIP_MODEL).to(config.DEVICE).eval()\n",
    "clip_processor = CLIPProcessor.from_pretrained(config.CLIP_MODEL)\n",
    "\n",
    "def path_for(img_id):\n",
    "    if \".\" not in img_id:\n",
    "        return os.path.join(config.IMAGE_DIR, f\"{img_id}.jpg\")\n",
    "    return os.path.join(config.IMAGE_DIR, img_id)\n",
    "\n",
    "@torch.no_grad()\n",
    "def rerank_images_clip(query, image_ids, top_k):\n",
    "    text_inputs = clip_processor.tokenizer(\n",
    "        query,\n",
    "        truncation=True,\n",
    "        max_length=config.MAX_CLIP_TOKENS,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    query = clip_processor.tokenizer.decode(text_inputs[\"input_ids\"][0], skip_special_tokens=True)\n",
    "    images = []\n",
    "    valid = []\n",
    "    for iid in image_ids:\n",
    "        p = path_for(iid)\n",
    "        try:\n",
    "            images.append(Image.open(p).convert(\"RGB\"))\n",
    "            valid.append(iid)\n",
    "        except:\n",
    "            pass\n",
    "    if not images:\n",
    "        return [\"#\"] * top_k\n",
    "    text_inputs = clip_processor(text=[query], return_tensors=\"pt\").to(config.DEVICE)\n",
    "    text_emb = clip_model.get_text_features(**text_inputs)\n",
    "    all_embs = []\n",
    "    for i in range(0, len(images), config.IMG_BATCH):\n",
    "        batch_imgs = images[i:i+config.IMG_BATCH]\n",
    "        inputs = clip_processor(images=batch_imgs, return_tensors=\"pt\", padding=True).to(config.DEVICE)\n",
    "        img_feats = clip_model.get_image_features(**inputs)\n",
    "        all_embs.append(img_feats)\n",
    "    img_emb = torch.cat(all_embs, dim=0)\n",
    "    text_emb = text_emb / text_emb.norm(dim=-1, keepdim=True)\n",
    "    img_emb = img_emb / img_emb.norm(dim=-1, keepdim=True)\n",
    "    sims = (img_emb @ text_emb.T).squeeze(-1).cpu().numpy()\n",
    "    ranked = sorted(zip(valid, sims), key=lambda x: x[1], reverse=True)\n",
    "    return [iid for iid, _ in ranked[:top_k]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD PRECOMPUTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "database_embeddings = np.load(config.EMBEDDINGS_FILE)\n",
    "database_article_ids = np.load(config.ARTICLE_IDS_FILE, allow_pickle=True).tolist()\n",
    "faiss_index = faiss.read_index(config.FAISS_INDEX_FILE)\n",
    "embedding_model = load_embedding_model()\n",
    "print(\"Models loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFERENCE - VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "val_queries = val_df['caption'].tolist()\n",
    "val_gt_articles = val_df['retrieved_article_id'].astype(str).tolist()\n",
    "val_gt_images = val_df['retrieved_image_id'].astype(str).tolist()\n",
    "\n",
    "Q = generate_embeddings(embedding_model, val_queries, config.EMBEDDING_BATCH_SIZE)\n",
    "Q = normalize_embeddings(Q)\n",
    "\n",
    "_, idx = search_index(faiss_index, Q, config.TOP_K_ARTICLES)\n",
    "candidates = [[database_article_ids[i] for i in row] for row in idx]\n",
    "\n",
    "reranked_articles = []\n",
    "for i in tqdm(range(0, len(val_queries), config.RERANK_BATCH_SIZE)):\n",
    "    batch_queries = val_queries[i:i+config.RERANK_BATCH_SIZE]\n",
    "    batch_candidates = candidates[i:i+config.RERANK_BATCH_SIZE]\n",
    "    batch_results = []\n",
    "    for q, c in zip(batch_queries, batch_candidates):\n",
    "        batch_results.append(rerank_articles(q, c, batch_size=4))\n",
    "    reranked_articles.extend(batch_results)\n",
    "\n",
    "final_images = []\n",
    "for q, arts in tqdm(zip(val_queries, reranked_articles), total=len(val_queries)):\n",
    "    imgs = []\n",
    "    for a in arts:\n",
    "        imgs.extend(article_image_map[a])\n",
    "    final_images.append(rerank_images_clip(q, imgs, config.TOP_K_IMAGES))\n",
    "\n",
    "print(\"Eval Article:\")\n",
    "article_metrics = evaluate_retrieval(reranked_articles, val_gt_articles, \"Article Retrieval\")\n",
    "print(\"Eval Image:\")\n",
    "image_metrics = evaluate_retrieval(final_images, val_gt_images, \"Image Retrieval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "k_vals = [1, 5, 10, 20, 50]\n",
    "article_recalls = [article_metrics[f\"Recall@{k}\"] for k in k_vals]\n",
    "axes[0, 0].plot(k_vals, article_recalls, marker='o', linewidth=2)\n",
    "axes[0, 0].set_title(\"Article Retrieval: Recall@K\", fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel(\"K\")\n",
    "axes[0, 0].set_ylabel(\"Recall\")\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "image_recalls = [image_metrics[f\"Recall@{k}\"] for k in k_vals]\n",
    "axes[0, 1].plot(k_vals, image_recalls, marker='s', linewidth=2)\n",
    "axes[0, 1].set_title(\"Image Retrieval: Recall@K\", fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel(\"K\")\n",
    "axes[0, 1].set_ylabel(\"Recall\")\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "article_summary_keys = [\"mAP\", \"MRR\", \"Recall@10\"]\n",
    "article_summary_vals = [article_metrics[k] for k in article_summary_keys]\n",
    "axes[1, 0].bar(article_summary_keys, article_summary_vals, alpha=0.7)\n",
    "axes[1, 0].set_title(\"Article Retrieval Summary\", fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylim([0, 1])\n",
    "axes[1, 0].set_ylabel(\"Score\")\n",
    "image_summary_keys = [\"mAP\", \"MRR\", \"Recall@10\"]\n",
    "image_summary_vals = [image_metrics[k] for k in image_summary_keys]\n",
    "axes[1, 1].bar(image_summary_keys, image_summary_vals, alpha=0.7)\n",
    "axes[1, 1].set_title(\"Image Retrieval Summary\", fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_ylim([0, 1])\n",
    "axes[1, 1].set_ylabel(\"Score\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"qwen3_results.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Visualization saved to qwen3_results.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST INFERENCE & SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(config.TEST_CSV)\n",
    "test_queries = test_df['query_text'].tolist()\n",
    "test_ids = test_df['query_index'].tolist()\n",
    "\n",
    "Q = generate_embeddings(embedding_model, test_queries, batch=config.EMBEDDING_BATCH_SIZE)\n",
    "Q = normalize_embeddings(Q)\n",
    "_, idx = search_index(faiss_index, Q, config.TOP_K_ARTICLES)\n",
    "test_candidates = [[database_article_ids[i] for i in row] for row in idx]\n",
    "\n",
    "test_articles = []\n",
    "for i in tqdm(range(0, len(test_queries), config.RERANK_BATCH_SIZE)):\n",
    "    batch_queries = test_queries[i:i+config.RERANK_BATCH_SIZE]\n",
    "    batch_candidates = test_candidates[i:i+config.RERANK_BATCH_SIZE]\n",
    "    batch_results = []\n",
    "    for q, c in zip(batch_queries, batch_candidates):\n",
    "        batch_results.append(rerank_articles(q, c, batch_size=4))\n",
    "    test_articles.extend(batch_results)\n",
    "\n",
    "test_images = []\n",
    "for q, arts in tqdm(zip(test_queries, test_articles), total=len(test_queries)):\n",
    "    imgs = []\n",
    "    for a in arts:\n",
    "        imgs.extend(article_image_map[a])\n",
    "    test_images.append(rerank_images_clip(q, imgs, config.TOP_K_IMAGES))\n",
    "\n",
    "rows = []\n",
    "for qid, imgs in zip(test_ids, test_images):\n",
    "    row = [qid] + imgs + [\"#\"]*(config.TOP_K_IMAGES-len(imgs))\n",
    "    rows.append(row)\n",
    "\n",
    "sub = pd.DataFrame(rows, columns=[\"query_id\"]+[f\"image_id_{i+1}\" for i in range(config.TOP_K_IMAGES)])\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Saved submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8524155,
     "sourceId": 13664156,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 498801,
     "modelInstanceId": 483272,
     "sourceId": 640787,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 499959,
     "modelInstanceId": 484473,
     "sourceId": 642441,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "adl_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
