{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INSTALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:55:57.193546Z",
     "iopub.status.busy": "2025-11-13T07:55:57.192730Z",
     "iopub.status.idle": "2025-11-13T07:56:00.767351Z",
     "shell.execute_reply": "2025-11-13T07:56:00.766313Z",
     "shell.execute_reply.started": "2025-11-13T07:55:57.193517Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install -q faiss-cpu sentence-transformers transformers torch torchvision accelerate safetensors networkx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:56:00.769528Z",
     "iopub.status.busy": "2025-11-13T07:56:00.769271Z",
     "iopub.status.idle": "2025-11-13T07:56:00.775888Z",
     "shell.execute_reply": "2025-11-13T07:56:00.775079Z",
     "shell.execute_reply.started": "2025-11-13T07:56:00.769503Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import faiss\n",
    "import pickle\n",
    "import warnings\n",
    "import numpy as np\n",
    "import igraph as ig\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict, deque\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import (\n",
    "    CLIPProcessor, CLIPModel,\n",
    "    AutoTokenizer, AutoModelForCausalLM\n",
    ")\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONSTANTS & CONFIG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:56:00.776898Z",
     "iopub.status.busy": "2025-11-13T07:56:00.776701Z",
     "iopub.status.idle": "2025-11-13T07:56:00.798400Z",
     "shell.execute_reply": "2025-11-13T07:56:00.797645Z",
     "shell.execute_reply.started": "2025-11-13T07:56:00.776883Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    DATASET_PATH: str = './Dataset'\n",
    "    DATABASE_JSON: str = f'{DATASET_PATH}/database.json'\n",
    "    TRAIN_CSV: str = f'{DATASET_PATH}/train_set.csv'\n",
    "    TEST_CSV: str = f'{DATASET_PATH}/test_public.csv'\n",
    "    IMAGE_DIR: str = f'{DATASET_PATH}/database_images_compressed90'\n",
    "\n",
    "    PRECOMPUTED_PATH: str = './eventa_embeddings_Qwen3'\n",
    "    EMBEDDINGS_FILE: str = f'{PRECOMPUTED_PATH}/database_embeddings_Qwen3.npy'\n",
    "    ARTICLE_IDS_FILE: str = f'{PRECOMPUTED_PATH}/database_article_ids_Qwen3.npy'\n",
    "    FAISS_INDEX_FILE: str = f'{PRECOMPUTED_PATH}/database_faiss_index_Qwen3.bin'\n",
    "\n",
    "    GRAPH_PATH: str = './hybrid_graphs_output/graph_hybrid.pkl'\n",
    "    NODE_MAPPING_PATH: str = './hybrid_graphs_output/node_mapping_hybrid.json'\n",
    "\n",
    "    EMBEDDING_MODEL: str = 'Qwen/Qwen3-Embedding-0.6B'\n",
    "    ARTICLE_RERANKER_MODEL: str = 'Qwen/Qwen3-Reranker-0.6B'\n",
    "    CLIP_MODEL: str = \"openai/clip-vit-large-patch14\"\n",
    "\n",
    "    DEVICE: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    EMBEDDING_BATCH_SIZE: int = 64\n",
    "    RERANK_BATCH_SIZE: int = 64\n",
    "    IMG_BATCH: int = 32\n",
    "\n",
    "    TOP_K_ARTICLES: int = 100\n",
    "    TOP_K_ARTICLES_RERANK: int = 20\n",
    "    TOP_K_IMAGES: int = 10\n",
    "\n",
    "    TOP_K_IMAGES_FAISS: int = 20\n",
    "\n",
    "    MAX_RERANKER_LENGTH: int = 4096\n",
    "    MAX_DOC_CHARS: int = 2000\n",
    "    MAX_CLIP_TOKENS: int = 75\n",
    "\n",
    "    GRAPH_HOPS: int = 5\n",
    "    MAX_GRAPH_CANDIDATES: int = 200\n",
    "\n",
    "    GRAPH_EXPANSION_MODE: str = \"weighted_hops\"\n",
    "    PER_NODE_TOPK: int = 20\n",
    "\n",
    "    TRAIN_VAL_SPLIT: float = 0.999\n",
    "    RANDOM_SEED: int = 42\n",
    "\n",
    "config = Config()\n",
    "print(\"Device:\", config.DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTILITY FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:56:00.800285Z",
     "iopub.status.busy": "2025-11-13T07:56:00.799977Z",
     "iopub.status.idle": "2025-11-13T07:56:00.815857Z",
     "shell.execute_reply": "2025-11-13T07:56:00.815009Z",
     "shell.execute_reply.started": "2025-11-13T07:56:00.800266Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_json(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def train_val_split(df, split_ratio=0.9, seed=42):\n",
    "    train = df.sample(frac=split_ratio, random_state=seed)\n",
    "    val = df.drop(train.index)\n",
    "    return train.reset_index(drop=True), val.reset_index(drop=True)\n",
    "\n",
    "def normalize_embeddings(E):\n",
    "    E = E.astype(np.float32)\n",
    "    faiss.normalize_L2(E)\n",
    "    return E\n",
    "\n",
    "def load_graph(graph_path, node_map_path):\n",
    "    with open(graph_path, 'rb') as f:\n",
    "        g = pickle.load(f)\n",
    "\n",
    "    with open(node_map_path, 'r', encoding='utf-8') as f:\n",
    "        node_map = json.load(f)\n",
    "\n",
    "    edge_list = g.get('edge_list', [])\n",
    "\n",
    "    adj = defaultdict(set)\n",
    "\n",
    "    adj_w = defaultdict(dict)\n",
    "\n",
    "    for a, b, w in edge_list:\n",
    "        a_i = int(a)\n",
    "        b_i = int(b)\n",
    "        w_f = float(w)\n",
    "        adj[a_i].add(b_i)\n",
    "        adj[b_i].add(a_i)\n",
    "\n",
    "        prev = adj_w[a_i].get(b_i, None)\n",
    "        if prev is None or w_f > prev:\n",
    "            adj_w[a_i][b_i] = w_f\n",
    "        prev = adj_w[b_i].get(a_i, None)\n",
    "        if prev is None or w_f > prev:\n",
    "            adj_w[b_i][a_i] = w_f\n",
    "\n",
    "    if \"sorted_index_to_article_id\" in node_map:\n",
    "        node_map = node_map[\"sorted_index_to_article_id\"]\n",
    "\n",
    "    idx_to_aid = {int(k): v for k, v in node_map.items()}\n",
    "    aid_to_idx = {v: int(k) for k, v in node_map.items()}\n",
    "\n",
    "    return adj, adj_w, idx_to_aid, aid_to_idx\n",
    "\n",
    "def expand_candidates_via_graph(\n",
    "    seed_article_ids,\n",
    "    graph_adj,\n",
    "    aid_to_idx,\n",
    "    idx_to_aid,\n",
    "    graph_weights=None,\n",
    "    mode=\"bfs\",\n",
    "    hops=1,\n",
    "    max_candidates=200,\n",
    "    topk_per_node=None\n",
    "):\n",
    "    if topk_per_node is None:\n",
    "        topk_per_node = config.PER_NODE_TOPK\n",
    "\n",
    "    seed_idxs = []\n",
    "    for aid in seed_article_ids:\n",
    "        if aid in aid_to_idx:\n",
    "            seed_idxs.append(aid_to_idx[aid])\n",
    "\n",
    "    if not seed_idxs:\n",
    "        return []\n",
    "\n",
    "    if mode == \"bfs\" or graph_weights is None:\n",
    "        visited = set(seed_idxs)\n",
    "        q = deque([(s, 0) for s in seed_idxs])\n",
    "        while q:\n",
    "            node, d = q.popleft()\n",
    "            if d >= hops:\n",
    "                continue\n",
    "            for nbr in graph_adj.get(node, []):\n",
    "                if nbr not in visited:\n",
    "                    visited.add(nbr)\n",
    "                    q.append((nbr, d+1))\n",
    "                if len(visited) >= max_candidates:\n",
    "                    break\n",
    "            if len(visited) >= max_candidates:\n",
    "                break\n",
    "\n",
    "        result = []\n",
    "        for idx in seed_idxs:\n",
    "            if idx in visited:\n",
    "                result.append(idx_to_aid[idx])\n",
    "        for idx in visited:\n",
    "            aid = idx_to_aid[idx]\n",
    "            if aid not in result:\n",
    "                result.append(aid)\n",
    "            if len(result) >= max_candidates:\n",
    "                break\n",
    "        return result[:max_candidates]\n",
    "\n",
    "    if mode == \"weighted_topk\":\n",
    "        score = defaultdict(float)\n",
    "        for s in seed_idxs:\n",
    "            nbrs = graph_weights.get(s, {})\n",
    "            for nbr, w in nbrs.items():\n",
    "                if nbr in seed_idxs:\n",
    "                    continue\n",
    "                score[nbr] += float(w)\n",
    "\n",
    "        ranked = sorted(score.items(), key=lambda x: x[1], reverse=True)\n",
    "        selected = [idx for idx, _ in ranked][:max_candidates]\n",
    "\n",
    "        result = []\n",
    "        for idx in seed_idxs:\n",
    "            if idx not in result:\n",
    "                result.append(idx_to_aid[idx])\n",
    "        for idx in selected:\n",
    "            aid = idx_to_aid.get(idx)\n",
    "            if aid is None:\n",
    "                continue\n",
    "            if aid not in result:\n",
    "                result.append(aid)\n",
    "            if len(result) >= max_candidates:\n",
    "                break\n",
    "        return result[:max_candidates]\n",
    "\n",
    "    if mode == \"weighted_hops\":\n",
    "        visited = set(seed_idxs)\n",
    "        q = deque([(s, 0) for s in seed_idxs])\n",
    "        while q:\n",
    "            node, d = q.popleft()\n",
    "            if d >= hops:\n",
    "                continue\n",
    "\n",
    "            nbr_w = graph_weights.get(node, None)\n",
    "            if nbr_w:\n",
    "                nbr_candidates = sorted(nbr_w.items(), key=lambda x: x[1], reverse=True)\n",
    "                nbr_candidates = [n for n, _ in nbr_candidates[:topk_per_node]]\n",
    "            else:\n",
    "                nbr_candidates = list(graph_adj.get(node, []))\n",
    "\n",
    "            for nbr in nbr_candidates:\n",
    "                if nbr not in visited:\n",
    "                    visited.add(nbr)\n",
    "                    q.append((nbr, d+1))\n",
    "                if len(visited) >= max_candidates:\n",
    "                    break\n",
    "            if len(visited) >= max_candidates:\n",
    "                break\n",
    "\n",
    "        result = []\n",
    "        for idx in seed_idxs:\n",
    "            if idx in visited:\n",
    "                result.append(idx_to_aid[idx])\n",
    "        for idx in visited:\n",
    "            aid = idx_to_aid[idx]\n",
    "            if aid not in result:\n",
    "                result.append(aid)\n",
    "            if len(result) >= max_candidates:\n",
    "                break\n",
    "        return result[:max_candidates]\n",
    "\n",
    "    return expand_candidates_via_graph(seed_article_ids, graph_adj, aid_to_idx, idx_to_aid,\n",
    "                                       graph_weights=None, mode=\"bfs\", hops=hops, max_candidates=max_candidates)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METRIC FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:56:00.817157Z",
     "iopub.status.busy": "2025-11-13T07:56:00.816789Z",
     "iopub.status.idle": "2025-11-13T07:56:00.841732Z",
     "shell.execute_reply": "2025-11-13T07:56:00.840871Z",
     "shell.execute_reply.started": "2025-11-13T07:56:00.817134Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_recall_at_k(pred, gt, k):\n",
    "    hit = sum(1 for p, g in zip(pred, gt) if g in p[:k])\n",
    "    return hit / len(gt)\n",
    "\n",
    "def compute_mrr(pred, gt):\n",
    "    s = []\n",
    "    for p, g in zip(pred, gt):\n",
    "        s.append(1/(p.index(g)+1) if g in p else 0)\n",
    "    return sum(s)/len(gt)\n",
    "\n",
    "def compute_map(pred, gt):\n",
    "    s = []\n",
    "    for p, g in zip(pred, gt):\n",
    "        s.append(1/(p.index(g)+1) if g in p else 0)\n",
    "    return sum(s)/len(gt)\n",
    "\n",
    "def evaluate_retrieval(pred, gt, name):\n",
    "    print(f\"\\n{name} Metrics:\")\n",
    "    metrics = {\n",
    "        \"mAP\": compute_map(pred, gt),\n",
    "        \"MRR\": compute_mrr(pred, gt),\n",
    "        \"Recall@1\": compute_recall_at_k(pred, gt, 1),\n",
    "        \"Recall@5\": compute_recall_at_k(pred, gt, 5),\n",
    "        \"Recall@10\": compute_recall_at_k(pred, gt, 10),\n",
    "        \"Recall@20\": compute_recall_at_k(pred, gt, 20),\n",
    "        \"Recall@50\": compute_recall_at_k(pred, gt, 50),\n",
    "    }\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATALOADER/DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:56:00.842979Z",
     "iopub.status.busy": "2025-11-13T07:56:00.842704Z",
     "iopub.status.idle": "2025-11-13T07:56:30.060839Z",
     "shell.execute_reply": "2025-11-13T07:56:30.059890Z",
     "shell.execute_reply.started": "2025-11-13T07:56:00.842940Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "database = load_json(config.DATABASE_JSON)\n",
    "train_df = pd.read_csv(config.TRAIN_CSV)\n",
    "train_df, val_df = train_val_split(train_df, config.TRAIN_VAL_SPLIT, config.RANDOM_SEED)\n",
    "\n",
    "article_image_map = defaultdict(list)\n",
    "for aid, data in database.items():\n",
    "    imgs = data.get(\"images\", []) or []\n",
    "    for img in imgs:\n",
    "        if isinstance(img, str):\n",
    "            iid = os.path.splitext(os.path.basename(img))[0]\n",
    "            article_image_map[aid].append(iid)\n",
    "            continue\n",
    "        if isinstance(img, dict):\n",
    "            for key in [\"image_id\",\"id\",\"file_name\",\"filename\",\"path\",\"file\"]:\n",
    "                if key in img:\n",
    "                    iid = os.path.splitext(os.path.basename(img[key]))[0]\n",
    "                    article_image_map[aid].append(iid)\n",
    "                    break\n",
    "\n",
    "print(\"Train:\", len(train_df), \"Val:\", len(val_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARCHITECTURE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMBEDDING & FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:56:30.062082Z",
     "iopub.status.busy": "2025-11-13T07:56:30.061778Z",
     "iopub.status.idle": "2025-11-13T07:56:30.081415Z",
     "shell.execute_reply": "2025-11-13T07:56:30.080598Z",
     "shell.execute_reply.started": "2025-11-13T07:56:30.062056Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_embedding_model():\n",
    "    m = SentenceTransformer(config.EMBEDDING_MODEL, device=config.DEVICE, trust_remote_code=True)\n",
    "    if config.DEVICE == 'cuda':\n",
    "        m.half()\n",
    "    return m\n",
    "\n",
    "def generate_embeddings(model, texts, batch=64):\n",
    "    out = []\n",
    "    for i in tqdm(range(0, len(texts), batch)):\n",
    "        b = model.encode(texts[i:i+batch], convert_to_numpy=True)\n",
    "        out.append(b)\n",
    "    return np.vstack(out)\n",
    "\n",
    "def search_index(index, Q, k):\n",
    "    Q = normalize_embeddings(Q)\n",
    "    dist, idx = index.search(Q, k)\n",
    "    return dist, idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QWEN3 RERANKER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:56:30.082431Z",
     "iopub.status.busy": "2025-11-13T07:56:30.082178Z",
     "iopub.status.idle": "2025-11-13T07:56:48.559134Z",
     "shell.execute_reply": "2025-11-13T07:56:48.558505Z",
     "shell.execute_reply.started": "2025-11-13T07:56:30.082405Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_qwen3_reranker():\n",
    "    tok = AutoTokenizer.from_pretrained(config.ARTICLE_RERANKER_MODEL, trust_remote_code=True, padding_side='left')\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.ARTICLE_RERANKER_MODEL,\n",
    "        torch_dtype=torch.float16 if config.DEVICE=='cuda' else torch.float32,\n",
    "        trust_remote_code=True\n",
    "    ).to(config.DEVICE)\n",
    "    model.eval()\n",
    "    prefix = \"<|im_start|>system\\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \\\"yes\\\" or \\\"no\\\".\\n<|im_end|>\\n<|im_start|>user\\n\"\n",
    "    suffix = \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\n",
    "    pre = tok.encode(prefix, add_special_tokens=False)\n",
    "    suf = tok.encode(suffix, add_special_tokens=False)\n",
    "    yes_id = tok.convert_tokens_to_ids(\"yes\") or tok.convert_tokens_to_ids(\"Yes\")\n",
    "    no_id  = tok.convert_tokens_to_ids(\"no\")  or tok.convert_tokens_to_ids(\"No\")\n",
    "    return tok, model, pre, suf, yes_id, no_id\n",
    "\n",
    "reranker_tokenizer, reranker_model, prefix_tokens, suffix_tokens, yes_id, no_id = load_qwen3_reranker()\n",
    "\n",
    "def format_instruction(query, doc):\n",
    "    return f\"<Instruct>: Determine relevance\\n<Query>: {query}\\n<Document>: {doc}\"\n",
    "\n",
    "def process_inputs(pairs):\n",
    "    toks = reranker_tokenizer(pairs, add_special_tokens=False, padding=False, truncation='longest_first')\n",
    "    for i, ids in enumerate(toks['input_ids']):\n",
    "        toks['input_ids'][i] = prefix_tokens + ids + suffix_tokens\n",
    "    toks = reranker_tokenizer.pad(\n",
    "        {\"input_ids\": toks['input_ids']},\n",
    "        padding=True, return_tensors=\"pt\",\n",
    "        max_length=config.MAX_RERANKER_LENGTH\n",
    "    )\n",
    "    return {k: v.to(config.DEVICE) for k, v in toks.items()}\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_yes_scores(inputs):\n",
    "    logits = reranker_model(**inputs).logits[:, -1, :]\n",
    "    pair = torch.stack([logits[:, no_id], logits[:, yes_id]], dim=1)\n",
    "    probs = torch.nn.functional.log_softmax(pair, dim=1)\n",
    "    return probs[:,1].exp().cpu().tolist()\n",
    "\n",
    "def rerank_articles(query, article_ids, batch_size=4):\n",
    "    pairs = []\n",
    "    for aid in article_ids:\n",
    "        art = database[aid]\n",
    "        title = art.get(\"title\", \"\")[:200]\n",
    "        content = art.get(\"content\", \"\")[:config.MAX_DOC_CHARS]\n",
    "        doc = f\"Title: {title}\\nContent: {content}\"\n",
    "        pairs.append(format_instruction(query, doc))\n",
    "    scores = []\n",
    "    for i in range(0, len(pairs), batch_size):\n",
    "        batch_pairs = pairs[i:i+batch_size]\n",
    "        inp = process_inputs(batch_pairs)\n",
    "        s = compute_yes_scores(inp)\n",
    "        scores.extend(s)\n",
    "        torch.cuda.empty_cache()\n",
    "    ranked = sorted(zip(article_ids, scores), key=lambda x: x[1], reverse=True)\n",
    "    return [a for a,_ in ranked[:config.TOP_K_ARTICLES_RERANK]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMAGE RERANKER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:56:48.560539Z",
     "iopub.status.busy": "2025-11-13T07:56:48.560287Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "clip_model = CLIPModel.from_pretrained(config.CLIP_MODEL).to(config.DEVICE).eval()\n",
    "clip_processor = CLIPProcessor.from_pretrained(config.CLIP_MODEL)\n",
    "\n",
    "def path_for(img_id):\n",
    "    if \".\" not in img_id:\n",
    "        return os.path.join(config.IMAGE_DIR, f\"{img_id}.jpg\")\n",
    "    return os.path.join(config.IMAGE_DIR, img_id)\n",
    "\n",
    "@torch.no_grad()\n",
    "def rerank_images_clip(query, image_ids, top_k):\n",
    "    text_inputs = clip_processor.tokenizer(\n",
    "        query,\n",
    "        truncation=True,\n",
    "        max_length=config.MAX_CLIP_TOKENS,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    query = clip_processor.tokenizer.decode(text_inputs[\"input_ids\"][0], skip_special_tokens=True)\n",
    "    images = []\n",
    "    valid = []\n",
    "    for iid in image_ids:\n",
    "        p = path_for(iid)\n",
    "        try:\n",
    "            images.append(Image.open(p).convert(\"RGB\"))\n",
    "            valid.append(iid)\n",
    "        except:\n",
    "            pass\n",
    "    if not images:\n",
    "        return [\"#\"] * top_k\n",
    "    text_inputs = clip_processor(text=[query], return_tensors=\"pt\").to(config.DEVICE)\n",
    "    text_emb = clip_model.get_text_features(**text_inputs)\n",
    "    all_embs = []\n",
    "    for i in range(0, len(images), config.IMG_BATCH):\n",
    "        batch_imgs = images[i:i+config.IMG_BATCH]\n",
    "        inputs = clip_processor(images=batch_imgs, return_tensors=\"pt\", padding=True).to(config.DEVICE)\n",
    "        img_feats = clip_model.get_image_features(**inputs)\n",
    "        all_embs.append(img_feats)\n",
    "    img_emb = torch.cat(all_embs, dim=0)\n",
    "    text_emb = text_emb / text_emb.norm(dim=-1, keepdim=True)\n",
    "    img_emb = img_emb / img_emb.norm(dim=-1, keepdim=True)\n",
    "    sims = (img_emb @ text_emb.T).squeeze(-1).cpu().numpy()\n",
    "    ranked = sorted(zip(valid, sims), key=lambda x: x[1], reverse=True)\n",
    "    return [iid for iid, _ in ranked[:top_k]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD PRECOMPUTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "database_embeddings = np.load(config.EMBEDDINGS_FILE, mmap_mode='r')\n",
    "database_article_ids = np.load(config.ARTICLE_IDS_FILE, allow_pickle=True).tolist()\n",
    "faiss_index = faiss.read_index(config.FAISS_INDEX_FILE, faiss.IO_FLAG_MMAP)\n",
    "embedding_model = load_embedding_model()\n",
    "\n",
    "graph_adj, graph_adj_weighted, idx_to_aid, aid_to_idx = load_graph(\n",
    "    config.GRAPH_PATH,\n",
    "    config.NODE_MAPPING_PATH\n",
    ")\n",
    "\n",
    "\n",
    "edges = []\n",
    "weights = []\n",
    "with open(config.GRAPH_PATH, 'rb') as f:\n",
    "    graw = pickle.load(f)\n",
    "\n",
    "for u, v, w in graw[\"edge_list\"]:\n",
    "    edges.append((int(u), int(v)))\n",
    "    weights.append(float(w))\n",
    "\n",
    "g_ig = ig.Graph(n=len(idx_to_aid), edges=edges)\n",
    "\n",
    "plt.hist(weights, bins=50)\n",
    "plt.title(\"Hybrid Graph Edge Weight Distribution\")\n",
    "plt.savefig(\"graph_edge_weight_distribution.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "num_components = len(g_ig.components())\n",
    "largest_comp = g_ig.components().giant()\n",
    "avg_path = largest_comp.average_path_length()\n",
    "\n",
    "print(\"Graph Connectivity (igraph):\")\n",
    "print(\"Components:\", num_components)\n",
    "print(\"Avg shortest path (largest component):\", avg_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFERENCE - VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "val_queries = val_df['caption'].tolist()\n",
    "val_gt_articles = val_df['retrieved_article_id'].astype(str).tolist()\n",
    "val_gt_images = val_df['retrieved_image_id'].astype(str).tolist()\n",
    "\n",
    "Q = generate_embeddings(embedding_model, val_queries, config.EMBEDDING_BATCH_SIZE)\n",
    "Q = normalize_embeddings(Q)\n",
    "\n",
    "_, idx = search_index(faiss_index, Q, config.TOP_K_ARTICLES)\n",
    "candidates = [[database_article_ids[i] for i in row] for row in idx]\n",
    "\n",
    "reranked_articles = []\n",
    "for i in tqdm(range(0, len(val_queries), config.RERANK_BATCH_SIZE)):\n",
    "    batch_queries = val_queries[i:i+config.RERANK_BATCH_SIZE]\n",
    "    batch_candidates = candidates[i:i+config.RERANK_BATCH_SIZE]\n",
    "    batch_results = []\n",
    "    for q, c in zip(batch_queries, batch_candidates):\n",
    "        expanded = expand_candidates_via_graph(\n",
    "            c,\n",
    "            graph_adj,\n",
    "            aid_to_idx,\n",
    "            idx_to_aid,\n",
    "            graph_weights=graph_adj_weighted,\n",
    "            mode=config.GRAPH_EXPANSION_MODE,\n",
    "            hops=config.GRAPH_HOPS,\n",
    "            max_candidates=config.MAX_GRAPH_CANDIDATES,\n",
    "            topk_per_node=config.PER_NODE_TOPK\n",
    "        )\n",
    "        batch_results.append(rerank_articles(q, expanded, batch_size=4))\n",
    "    reranked_articles.extend(batch_results)\n",
    "\n",
    "final_images = []\n",
    "for q, arts in tqdm(zip(val_queries, reranked_articles), total=len(val_queries)):\n",
    "    imgs = []\n",
    "    for a in arts:\n",
    "        imgs.extend(article_image_map.get(a, []))\n",
    "    final_images.append(rerank_images_clip(q, imgs, config.TOP_K_IMAGES))\n",
    "\n",
    "article_metrics = evaluate_retrieval(reranked_articles, val_gt_articles, \"Article Retrieval\")\n",
    "image_metrics = evaluate_retrieval(final_images, val_gt_images, \"Image Retrieval\")\n",
    "\n",
    "baseline_top1 = [c[0] for c in candidates]\n",
    "graph_top1 = [r[0] for r in reranked_articles]\n",
    "agree = [1 if a==b else 0 for a,b in zip(baseline_top1, graph_top1)]\n",
    "\n",
    "plt.scatter(range(len(agree)), agree, alpha=0.5)\n",
    "plt.title(\"Top-1 Agreement: Baseline vs Graph\")\n",
    "plt.savefig(\"scatter_top1_baseline_vs_graph.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "neighbor_counts = []\n",
    "for c in candidates:\n",
    "    exp = expand_candidates_via_graph(\n",
    "        c,\n",
    "        graph_adj,\n",
    "        aid_to_idx,\n",
    "        idx_to_aid,\n",
    "        graph_weights=graph_adj_weighted,\n",
    "        mode=config.GRAPH_EXPANSION_MODE,\n",
    "        hops=config.GRAPH_HOPS,\n",
    "        max_candidates=config.MAX_GRAPH_CANDIDATES,\n",
    "        topk_per_node=config.PER_NODE_TOPK\n",
    "    )\n",
    "    neighbor_counts.append(len(set(exp) - set(c)))\n",
    "\n",
    "plt.hist(neighbor_counts, bins=40)\n",
    "plt.title(\"Graph Neighbor Expansion Counts\")\n",
    "plt.savefig(\"graph_neighbor_overlap.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "qual = []\n",
    "for idx in np.random.choice(len(val_queries), 5, replace=False):\n",
    "    qual.append({\n",
    "        \"query\": val_queries[idx],\n",
    "        \"baseline_top5\": candidates[idx][:5],\n",
    "        \"graph_top5\": reranked_articles[idx][:5]\n",
    "    })\n",
    "\n",
    "pd.DataFrame(qual).to_csv(\"qualitative_graph_comparison.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "k_vals = [1, 5, 10, 20, 50]\n",
    "article_recalls = [article_metrics[f\"Recall@{k}\"] for k in k_vals]\n",
    "axes[0, 0].plot(k_vals, article_recalls, marker='o', linewidth=2)\n",
    "axes[0, 0].set_title(\"Article Retrieval: Recall@K\", fontsize=12, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "image_recalls = [image_metrics[f\"Recall@{k}\"] for k in k_vals]\n",
    "axes[0, 1].plot(k_vals, image_recalls, marker='s', linewidth=2)\n",
    "axes[0, 1].set_title(\"Image Retrieval: Recall@K\", fontsize=12, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "article_summary_keys = [\"mAP\", \"MRR\", \"Recall@10\"]\n",
    "article_summary_vals = [article_metrics[k] for k in article_summary_keys]\n",
    "axes[1, 0].bar(article_summary_keys, article_summary_vals, alpha=0.7)\n",
    "axes[1, 0].set_title(\"Article Retrieval Summary\", fontsize=12, fontweight='bold')\n",
    "\n",
    "image_summary_keys = [\"mAP\", \"MRR\", \"Recall@10\"]\n",
    "image_summary_vals = [image_metrics[k] for k in image_summary_keys]\n",
    "axes[1, 1].bar(image_summary_keys, image_summary_vals, alpha=0.7)\n",
    "axes[1, 1].set_title(\"Image Retrieval Summary\", fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"qwen3_results.png\", dpi=150)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST INFERENCE & SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(config.TEST_CSV)\n",
    "test_queries = test_df['query_text'].tolist()\n",
    "test_ids = test_df['query_index'].tolist()\n",
    "\n",
    "Q = generate_embeddings(embedding_model, test_queries, batch=config.EMBEDDING_BATCH_SIZE)\n",
    "Q = normalize_embeddings(Q)\n",
    "_, idx = search_index(faiss_index, Q, config.TOP_K_ARTICLES)\n",
    "test_candidates = [[database_article_ids[i] for i in row] for row in idx]\n",
    "\n",
    "test_articles = []\n",
    "for i in tqdm(range(0, len(test_queries), config.RERANK_BATCH_SIZE)):\n",
    "    batch_queries = test_queries[i:i+config.RERANK_BATCH_SIZE]\n",
    "    batch_candidates = test_candidates[i:i+config.RERANK_BATCH_SIZE]\n",
    "    batch_results = []\n",
    "    for q, c in zip(batch_queries, batch_candidates):\n",
    "        expanded = expand_candidates_via_graph(\n",
    "            c,\n",
    "            graph_adj,\n",
    "            aid_to_idx,\n",
    "            idx_to_aid,\n",
    "            graph_weights=graph_adj_weighted,\n",
    "            mode=config.GRAPH_EXPANSION_MODE,\n",
    "            hops=config.GRAPH_HOPS,\n",
    "            max_candidates=config.MAX_GRAPH_CANDIDATES,\n",
    "            topk_per_node=config.PER_NODE_TOPK\n",
    "        )\n",
    "        batch_results.append(rerank_articles(q, expanded, batch_size=4))\n",
    "    test_articles.extend(batch_results)\n",
    "\n",
    "test_images = []\n",
    "for q, arts in tqdm(zip(test_queries, test_articles), total=len(test_queries)):\n",
    "    imgs = []\n",
    "    for a in arts:\n",
    "        imgs.extend(article_image_map.get(a, []))\n",
    "    test_images.append(rerank_images_clip(q, imgs, config.TOP_K_IMAGES))\n",
    "\n",
    "rows = []\n",
    "for qid, imgs in zip(test_ids, test_images):\n",
    "    rows.append([qid] + imgs + [\"#\"]*(config.TOP_K_IMAGES-len(imgs)))\n",
    "\n",
    "sub = pd.DataFrame(rows, columns=[\"query_id\"]+[f\"image_id_{i+1}\" for i in range(config.TOP_K_IMAGES)])\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Saved submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8524155,
     "sourceId": 13664156,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 498801,
     "modelInstanceId": 483272,
     "sourceId": 640787,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 499959,
     "modelInstanceId": 484473,
     "sourceId": 642441,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "adl_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
